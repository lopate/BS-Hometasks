{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "FMNIST_train = torchvision.datasets.FashionMNIST('./fmnist', train=True, download = True, transform = torchvision.transforms.ToTensor())\n",
    "FMNIST_test = torchvision.datasets.FashionMNIST('./fmnist', train=False, download = False, transform = torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 15050), started 0:00:02 ago. (Use '!kill 15050' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-f4749b74ea781ecd\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-f4749b74ea781ecd\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/hparam_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-02 09:12:40.161266: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-02 09:12:40.161307: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-02 09:12:40.161313: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-02 09:12:40.165546: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.transforms import ToTensor, PILToTensor, Compose\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сформируем стандартные функции для обучения по батчу, по эпохам и обертку для инициализации оптимизатора и датасета для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_batch(model, x_batch, y_batch, optimizer, loss_function):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x_batch.to(model.device()))\n",
    "    loss = loss_function(output, y_batch.to(model.device()))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.cpu().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_epoch(train_generator, model, loss_function, optimizer, callback = None):\n",
    "    epoch_loss = 0\n",
    "    total = 0\n",
    "    for it, (batch_of_x, batch_of_y) in enumerate(train_generator):\n",
    "        batch_loss = train_on_batch(model, batch_of_x, batch_of_y, optimizer, loss_function)\n",
    "        \n",
    "        if callback is not None:\n",
    "            with torch.no_grad():\n",
    "                callback(model, batch_loss)\n",
    "            \n",
    "        epoch_loss += batch_loss*len(batch_of_x)\n",
    "        total += len(batch_of_x)\n",
    "    \n",
    "    return epoch_loss/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(count_of_epoch, \n",
    "            batch_size, \n",
    "            dataset,\n",
    "            model, \n",
    "            loss_function,\n",
    "            optimizer,\n",
    "            lr = 0.001,\n",
    "            callback = None):\n",
    "\n",
    "    optima = optimizer(model.parameters(), lr=lr)\n",
    "\n",
    "    iterations = tqdm.tqdm(range(count_of_epoch), desc='epoch')\n",
    "    iterations.set_postfix({'train epoch loss': np.nan})\n",
    "    epoch_loss = None\n",
    "    for it in iterations:\n",
    "        batch_generator = tqdm.tqdm(\n",
    "            torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True), \n",
    "            leave=False, total=len(dataset)//batch_size+(len(dataset)%batch_size> 0))\n",
    "        \n",
    "        epoch_loss = train_epoch(train_generator=batch_generator, \n",
    "                    model=model, \n",
    "                    loss_function=loss_function, \n",
    "                    optimizer=optima, \n",
    "                    callback=callback)\n",
    "        \n",
    "        iterations.set_postfix({'train epoch loss': epoch_loss})\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Блоки энкодера CNN (DownBlock) сформируем на основе двух сверточных слоев, каждый из которых состоит из собственно свертки, потом нормализации по батчу ,если используется, и функции активации. Также после функции активации для кадого слоя кроме последнего, стоит Dropout с заданным параметром. Полсе последнего слоя свертки стоит пуллинг. Т.к. по заданию нам требуется менять струкутру сети, то для удобства каждый сверточный слой формируем с помощью nn.Sequential, что позволит добавлять в него модули по требованию. Для простоты определения параметров используем паддинг."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, down=True, dropout = 0.5, kernel_size = 3, batch_norm = True, pooling ='maxpool'):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential()\n",
    "        self.conv1.add_module('convl1',nn.Conv2d(in_channel, out_channel, kernel_size, padding='same', padding_mode = 'reflect')),\n",
    "        if batch_norm:\n",
    "            self.conv1.add_module('batch1',nn.BatchNorm2d(out_channel)),\n",
    "        self.conv1.add_module('relu1',nn.ReLU()),\n",
    "        self.conv1.add_module('dropout1',nn.Dropout(dropout))\n",
    "\n",
    "        self.conv2 = nn.Sequential()\n",
    "        self.conv2.add_module('convl2',nn.Conv2d(out_channel, out_channel, kernel_size, padding='same', padding_mode = 'reflect')),\n",
    "        if batch_norm:\n",
    "            self.conv2.add_module('batch2', nn.BatchNorm2d(out_channel)),\n",
    "        self.conv2.add_module('relu2', nn.ReLU())\n",
    "\n",
    "        if pooling == 'maxpool':\n",
    "            self.pooling = torch.nn.MaxPool2d(2)\n",
    "        elif 'avgpool':\n",
    "            self.pooling = torch.nn.AvgPool2d(2)\n",
    "        \n",
    "        # TODO\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pooling(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классификацию мы проводим на ветораях эмбедингов полученных от энкодера, у нас стоит задача классификации, а не восстановления исходных объектов. Поэтому слои декодера определять не имеет смысла."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определеим общую структуру нашей сети. Первые два сверточных слоя сильно отличаются по структуре от пердыдущих, поэтому их придется определить вручную. Параметры остальных слоев получаются из предыдущих делением на 2, т.к. мы использовали паддинг до нужных размеров. Выход от сверточных слоев мы подаем на вход полносвязанной сети как веторы, которая используется как классификационная сеть. На выходе полносвяанной сети соотвественно получаем вероятности каждого класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "    def __init__(self,\n",
    "                 num_classes, \n",
    "                 min_channels=32,\n",
    "                 max_channels=512,\n",
    "                 colors = 1,\n",
    "                 num_down_blocks=4, image_size = 32, kernel_size = 3, dropout = 0.2, batch_norm = True, pooling ='maxpool'):\n",
    "        super(CNN, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_down_blocks = num_down_blocks\n",
    "        self.conv1 = nn.Sequential()\n",
    "        self.conv1.add_module('convl1', nn.Conv2d(colors, min_channels, kernel_size, padding='same', padding_mode = 'reflect')),\n",
    "        if batch_norm:\n",
    "            self.conv1.add_module('batch1',nn.BatchNorm2d(min_channels)),\n",
    "        self.conv1.add_module('relu1', nn.ReLU()),\n",
    "        self.conv1.add_module('dropout1', nn.Dropout(dropout))\n",
    "\n",
    "        self.conv2 = nn.Sequential()\n",
    "        self.conv2.add_module('conv2', nn.Conv2d(min_channels, min_channels, kernel_size, padding='same', padding_mode = 'reflect')),\n",
    "        if batch_norm:\n",
    "            self.conv2.add_module('batch2', nn.BatchNorm2d(min_channels)),\n",
    "        self.conv2.add_module('relu2', nn.ReLU())\n",
    "\n",
    "        \n",
    "        self.down_blocks = nn.ModuleList()\n",
    "        prev_chanel = min_channels\n",
    "        self.chanel_size = []\n",
    "        for i in range(self.num_down_blocks):\n",
    "            self.chanel_size.append(prev_chanel)\n",
    "            out_chanel = prev_chanel * 2\n",
    "            self.down_blocks.append(DownBlock(prev_chanel, out_chanel, dropout=dropout, kernel_size = 3, batch_norm = True, pooling ='maxpool'))\n",
    "            prev_chanel = out_chanel\n",
    "        #print((image_size + (2 << (num_down_blocks - 1)) - 1))\n",
    "        multiplier =  ((image_size + (2 << (num_down_blocks - 1)) - 1)>> num_down_blocks)\n",
    "        #print(multiplier)\n",
    "        if multiplier > 0:\n",
    "            prev_chanel *= multiplier**2\n",
    "        #print(prev_chanel)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.linerlayer1 = nn.Sequential(\n",
    "            nn.Linear(prev_chanel, int(prev_chanel / 4)), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.linerlayer2 = nn.Sequential(\n",
    "            nn.Linear(int(prev_chanel / 4), int(prev_chanel / 8)), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.linerlayer3 = nn.Sequential(\n",
    "            nn.Linear(int(prev_chanel / 8), num_classes)\n",
    "        )\n",
    "        self.tensordevice = nn.Parameter()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # TODO\n",
    "        width = inputs.shape[-1]\n",
    "        height = inputs.shape[-2]\n",
    "        width_ = 2 ** self.num_down_blocks - ((width - 1) % (2 ** self.num_down_blocks) + 1)\n",
    "        height_ = 2 ** self.num_down_blocks- ((height- 1) % (2 ** self.num_down_blocks)  + 1)\n",
    "        padding = torch.nn.ReflectionPad2d((0, width_, 0, height_))\n",
    "        x = padding(inputs)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x_skip = [x]\n",
    "        for i in range(self.num_down_blocks):\n",
    "            x = self.down_blocks[i](x)\n",
    "        #print(x.shape)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linerlayer1(x)\n",
    "        x = self.linerlayer2(x)\n",
    "        x = self.linerlayer3(x)\n",
    "        logits = x\n",
    "        #assert logits.shape == (inputs.shape[0], self.num_classes, inputs.shape[2], inputs.shape[3]), 'Wrong shape of the logits'\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(dataset=FMNIST_train, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN(10, min_channels=4, image_size = 28, colors = 1, num_down_blocks=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве функции потерь для задачи классификации возьмем кросс-энтропию, в качестве оптимизатора Adam без регуляризации (weight decay)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем на GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Sequential(\n",
       "    (convl1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=same, padding_mode=reflect)\n",
       "    (batch1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu1): ReLU()\n",
       "    (dropout1): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same, padding_mode=reflect)\n",
       "    (batch2): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       "  (down_blocks): ModuleList(\n",
       "    (0): DownBlock(\n",
       "      (conv1): Sequential(\n",
       "        (convl1): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1), padding=same, padding_mode=reflect)\n",
       "        (batch1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU()\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (convl2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=same, padding_mode=reflect)\n",
       "        (batch2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU()\n",
       "      )\n",
       "      (pooling): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (1): DownBlock(\n",
       "      (conv1): Sequential(\n",
       "        (convl1): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=same, padding_mode=reflect)\n",
       "        (batch1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU()\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (convl2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=same, padding_mode=reflect)\n",
       "        (batch2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU()\n",
       "      )\n",
       "      (pooling): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (up_blocks): ModuleList()\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linerlayer1): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=196, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (linerlayer2): Sequential(\n",
       "    (0): Linear(in_features=196, out_features=98, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (linerlayer3): Sequential(\n",
       "    (0): Linear(in_features=98, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trainer(20, \\n        64, \\n        FMNIST_train,\\n        cnn, \\n        torch.nn.CrossEntropyLoss(),\\n        torch.optim.Adam,\\n        lr = 0.001\\n        )'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"trainer(20, \n",
    "        64, \n",
    "        FMNIST_train,\n",
    "        cnn, \n",
    "        torch.nn.CrossEntropyLoss(),\n",
    "        torch.optim.Adam,\n",
    "        lr = 0.001\n",
    "        )\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для получения логов используем tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для логгирования всех метрик на этапе обучения лучше всего использщовать отдельный интерфейс. В нем мы каждый шаг записываем значения loss на трейне. И раз в несколько сотен шаг записывем значение метрик на валидационной выборке. Что позволяет отслеживать темп обучения и его качетсво, при этом не сильно теряя во времени обучения "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class callback():\n",
    "    def __init__(self, writer, dataset, loss_function, metrics, hparams,  delimeter = 100, batch_size=64):\n",
    "        self.step = 0\n",
    "        self.writer = writer\n",
    "        self.delimeter = delimeter\n",
    "        self.loss_function = loss_function\n",
    "        self.batch_size = batch_size\n",
    "        self.hparams = hparams\n",
    "        self.dataset = dataset\n",
    "        self.metrics = metrics\n",
    "\n",
    "    def forward(self, model, loss):\n",
    "        self.step += 1\n",
    "        with self.writer.as_default():\n",
    "            hp.hparams(self.hparams)\n",
    "            tf.summary.scalar(self.metrics + '/train', loss, self.step)\n",
    "        \n",
    "        if self.step % self.delimeter == 0:\n",
    "            \n",
    "            batch_generator = torch.utils.data.DataLoader(dataset = self.dataset, \n",
    "                                                          batch_size=self.batch_size)\n",
    "            \n",
    "            pred = []\n",
    "            real = []\n",
    "            test_loss = 0\n",
    "            model.eval()\n",
    "            for it, (x_batch, y_batch) in enumerate(batch_generator):\n",
    "                x_batch = x_batch.to(model.device())\n",
    "\n",
    "                output = model(x_batch)\n",
    "\n",
    "                test_loss += self.loss_function(output, y_batch.to(model.device())).cpu().item()*len(x_batch)\n",
    "\n",
    "                pred.extend(torch.argmax(output, dim=-1).cpu().numpy().tolist())\n",
    "            \n",
    "            test_loss /= len(self.dataset)\n",
    "            with self.writer.as_default():\n",
    "                tf.summary.scalar(self.metrics + '/test', test_loss, self.step)\n",
    "\n",
    "            x = x_batch[-10:]\n",
    "          \n",
    "    def __call__(self, model, loss):\n",
    "        return self.forward(model, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве алогритма по перебору гиперпараметров больше всего приглянулась optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import optuna_dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим сетку по которй будем перебирать гиперпараметры, по заданию не требуется перебирать все варианты, поэтому построим набольшую сетку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_KERNEL_SIZE = hp.HParam('num_units', hp.IntInterval(2, 5))\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.3, 0.5))\n",
    "HP_POOLING = hp.HParam('poolling', hp.Discrete(['maxpool', 'avgpool']))\n",
    "HP_BATCH_NORM = hp.HParam('batch_norm', hp.Discrete([True, False]))\n",
    "HP_LAYERS_COUNT = hp.HParam('layers_count', hp.IntInterval(2, 5))\n",
    "METRICS_NAME = \"LOSS\"\n",
    "DELIMETER = 100\n",
    "IMAGE_SIZE = 28\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE  = 128\n",
    "COLORS = 1\n",
    "MIN_CHANNELS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это нужно для логгирования каждого значения гиперпараметра в tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_dir = 'logs/hparam_tuning'\n",
    "writer_hparam = tf.summary.create_file_writer('logs/hparam_tuning')\n",
    "with writer_hparam.as_default():\n",
    "  hp.hparams_config(\n",
    "    hparams=[HP_KERNEL_SIZE, HP_DROPOUT, HP_POOLING, HP_BATCH_NORM, HP_LAYERS_COUNT],\n",
    "    metrics=[hp.Metric(METRICS_NAME  + '/train', display_name='Loss')],\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец запустим наше обучение. На вход optuna нужно передать функцию, которая по интерфейсу trial (соответсвует одной попытке, из которой можно получить значение гиперпараметров предлагаемых optnua) выдает значение оптимизируемой метрики. На выходе она выдаетрезультаты лучшей попытки попытки, оптимальные значение гиперпараметров. А также позволяет визуализировать значение гиперпараметров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-02 09:44:46,505] A new study created in memory with name: no-name-3dac8b69-f95d-4b39-9ec8-66a1d3e0721a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current run: logs/hparam_tuning/run-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 10/10 [05:17<00:00, 31.75s/it, train epoch loss=0.252]\n",
      "[I 2024-04-02 09:50:04,014] Trial 0 finished with value: 0.25240968391100566 and parameters: {'num_units': 4, 'dropout': 0.4019725961076505, 'poolling': 'maxpool', 'batch_norm': True, 'layers_count': 2}. Best is trial 0 with value: 0.25240968391100566.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25240968391100566\n",
      "Current run: logs/hparam_tuning/run-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 10/10 [06:15<00:00, 37.55s/it, train epoch loss=0.347]\n",
      "[I 2024-04-02 09:56:19,495] Trial 1 finished with value: 0.34670891030629475 and parameters: {'num_units': 4, 'dropout': 0.4040352602677366, 'poolling': 'avgpool', 'batch_norm': False, 'layers_count': 5}. Best is trial 1 with value: 0.34670891030629475.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34670891030629475\n",
      "Current run: logs/hparam_tuning/run-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 10/10 [05:48<00:00, 34.87s/it, train epoch loss=0.284]\n",
      "[I 2024-04-02 10:02:08,167] Trial 2 finished with value: 0.28366628330548604 and parameters: {'num_units': 3, 'dropout': 0.3020975984652778, 'poolling': 'avgpool', 'batch_norm': False, 'layers_count': 4}. Best is trial 1 with value: 0.34670891030629475.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28366628330548604\n",
      "Current run: logs/hparam_tuning/run-3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 10/10 [06:18<00:00, 37.81s/it, train epoch loss=0.356]\n",
      "[I 2024-04-02 10:08:26,250] Trial 3 finished with value: 0.35567838447888694 and parameters: {'num_units': 2, 'dropout': 0.44505937658395445, 'poolling': 'avgpool', 'batch_norm': False, 'layers_count': 5}. Best is trial 3 with value: 0.35567838447888694.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35567838447888694\n",
      "Current run: logs/hparam_tuning/run-4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 10/10 [06:17<00:00, 37.76s/it, train epoch loss=0.335]\n",
      "[I 2024-04-02 10:14:43,910] Trial 4 finished with value: 0.3345901206334432 and parameters: {'num_units': 5, 'dropout': 0.37507909939402145, 'poolling': 'maxpool', 'batch_norm': False, 'layers_count': 5}. Best is trial 3 with value: 0.35567838447888694.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3345901206334432\n",
      "Current run: logs/hparam_tuning/run-5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 10/10 [06:18<00:00, 37.82s/it, train epoch loss=0.314]\n",
      "[I 2024-04-02 10:21:02,109] Trial 5 finished with value: 0.3139294596354167 and parameters: {'num_units': 3, 'dropout': 0.35787075002188623, 'poolling': 'avgpool', 'batch_norm': True, 'layers_count': 5}. Best is trial 3 with value: 0.35567838447888694.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3139294596354167\n",
      "Current run: logs/hparam_tuning/run-6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 10/10 [06:13<00:00, 37.31s/it, train epoch loss=0.351]\n",
      "[I 2024-04-02 10:27:15,210] Trial 6 finished with value: 0.35102645848592123 and parameters: {'num_units': 4, 'dropout': 0.41563831409385066, 'poolling': 'maxpool', 'batch_norm': False, 'layers_count': 5}. Best is trial 3 with value: 0.35567838447888694.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35102645848592123\n",
      "Current run: logs/hparam_tuning/run-7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 10/10 [05:57<00:00, 35.72s/it, train epoch loss=0.347]\n",
      "[I 2024-04-02 10:33:12,392] Trial 7 finished with value: 0.34661933118502297 and parameters: {'num_units': 2, 'dropout': 0.4040403525869327, 'poolling': 'maxpool', 'batch_norm': True, 'layers_count': 4}. Best is trial 3 with value: 0.35567838447888694.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34661933118502297\n",
      "Current run: logs/hparam_tuning/run-8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 10/10 [06:09<00:00, 36.97s/it, train epoch loss=0.316]\n",
      "[I 2024-04-02 10:39:22,117] Trial 8 finished with value: 0.31561945128440855 and parameters: {'num_units': 3, 'dropout': 0.3522595966607735, 'poolling': 'maxpool', 'batch_norm': False, 'layers_count': 5}. Best is trial 3 with value: 0.35567838447888694.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31561945128440855\n",
      "Current run: logs/hparam_tuning/run-9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 10/10 [06:23<00:00, 38.37s/it, train epoch loss=0.349]\n",
      "[I 2024-04-02 10:45:45,818] Trial 9 finished with value: 0.3490602419694265 and parameters: {'num_units': 3, 'dropout': 0.4435010060686884, 'poolling': 'avgpool', 'batch_norm': True, 'layers_count': 5}. Best is trial 3 with value: 0.35567838447888694.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3490602419694265\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def objective(trial):\n",
    "    run_number = trial.number\n",
    "    run_name = logs_dir + f'/run-{run_number}'\n",
    "    print(\"Current run: \" + run_name)\n",
    "    kernel_size =  trial.suggest_int('num_units', 2, 5)\n",
    "    dropout = trial.suggest_float('dropout', 0.3, 0.5)\n",
    "    pooling = trial.suggest_categorical('poolling', ['maxpool', 'avgpool'])\n",
    "    batch_norm = trial.suggest_categorical('batch_norm', [True, False])\n",
    "    layers_count = trial.suggest_int('layers_count', 2, 5)\n",
    "    writer = tf.summary.create_file_writer(run_name)\n",
    "    cnn = CNN(10, min_channels=MIN_CHANNELS , colors = COLORS , image_size=IMAGE_SIZE,  num_down_blocks=layers_count, kernel_size=kernel_size, dropout=dropout, pooling=pooling, batch_norm=batch_norm)\n",
    "    cnn.cuda()\n",
    "    hparams = {\n",
    "      HP_KERNEL_SIZE: kernel_size,\n",
    "      HP_DROPOUT: dropout,\n",
    "      HP_POOLING:  pooling,\n",
    "      HP_BATCH_NORM: batch_norm,\n",
    "      HP_LAYERS_COUNT: layers_count,\n",
    "    }\n",
    "    loss = trainer(EPOCHS, \n",
    "        BATCH_SIZE, \n",
    "        FMNIST_train,\n",
    "        cnn, \n",
    "        torch.nn.CrossEntropyLoss(),\n",
    "        torch.optim.Adam,\n",
    "        lr = 0.001,\n",
    "        callback = callback(writer, FMNIST_train, torch.nn.CrossEntropyLoss(), METRICS_NAME , hparams, DELIMETER))\n",
    "    print(loss)\n",
    "    return loss\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вывод\n",
    "По результатам переборов гиперпараметров maxpool показывает себя лучше, чем avgpool. Что объяснентся тем, что maxpool делает свертку инвариантной относительно расположения интересующей части картинки. Т.е. для maxpool по потсроению не важно в какой части изображения находится классифицируемый объект. Ожидаемо нормализация также улучшила качество классификации. Также благоприятно сказывется на обучении увеличение кол-во слоев. А вот зависимоти от dropout не выявилось, скорее всего оптимальные значения лежат в пределе исследуемых.\n",
    "\n",
    "В целом результаты получились ожидаемые пуллинг по максимуму оказался лучше, чем усредняющий, т.к. выделяет фичи из интересующей части изображения, а не усредняет их по всем. Разумное увеличение количества слоев таже увеличвет качество определяемых фичей и соотвественно качество классификации. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
