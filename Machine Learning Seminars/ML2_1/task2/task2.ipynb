{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nerus in /home/sasha/anaconda3/lib/python3.11/site-packages (1.7.0)\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install nerus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим датасет NERUS через предложенный интерфейс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEURUS = './nerus_lenta.conllu.gz'\n",
    "from nerus import load_nerus\n",
    "docs = load_nerus(NEURUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = next(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим как выглядит элемент датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NerusToken(\n",
       "    id='1',\n",
       "    text='Вице-премьер',\n",
       "    pos='NOUN',\n",
       "    feats={'Animacy': 'Anim',\n",
       "     'Case': 'Nom',\n",
       "     'Gender': 'Masc',\n",
       "     'Number': 'Sing'},\n",
       "    head_id='7',\n",
       "    rel='nsubj',\n",
       "    tag='O'\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.sents[0].tokens[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим класс словаря для pos_tags. Для каждого встретившегося нового тега используем следующий новый номер. Не забудем добавить технические теги для начала, конца и паддинга."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosesVocabulary:\n",
    "    def __init__(self):\n",
    "        self.idx2pos = {0: '[PAD]', 1: '[CLS]', 2: '[SEP]'}\n",
    "        self.pos2idx = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2}\n",
    "        self.pos_tags = ['ADJ', 'ADV', 'INTJ', 'NOUN', 'PROPN',\n",
    "                         'VERB', 'ADP', 'AUX', 'CCONJ', 'DET', 'NUM',\n",
    "                         'PART', 'PRON', 'SCONJ', 'PUNCT', 'SYM', 'X']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2pos)\n",
    "\n",
    "    def build_vocabulary(self):\n",
    "        idx = 3\n",
    "        for pos in self.pos_tags:\n",
    "            self.idx2pos[idx] = pos\n",
    "            self.pos2idx[pos] = idx\n",
    "            idx += 1\n",
    "\n",
    "    def numericalize(self, poses):\n",
    "        return [self.pos2idx[pos] for pos in poses]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В целом тот же самый словарь делаем и для слов. Единственное для задание нужно уметь изменять размер словаря. Для этого мы используем кучу, в которй будем хранить чило раз, когда каждое слово встретилось. Соотвественно, если мы хотим сделать соварь из n слов, то берем n самых частых слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "class WordsVocabulary:\n",
    "    def __init__(self, freq_threshold):\n",
    "        self.idx2word = {0: '[PAD]', 1: '[CLS]', 2: '[SEP]', 3: '[UNK]'}\n",
    "        self.word2idx = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[UNK]': 3}\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.frequencies = {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "\n",
    "    def build_vocabulary(self, sents, voc_size = None):\n",
    "        idx = len(self.word2idx)\n",
    "        for sent in sents:\n",
    "            for word in sent:\n",
    "                if word not in self.frequencies:\n",
    "                    self.frequencies[word] = 1\n",
    "                else:\n",
    "                    self.frequencies[word] += 1\n",
    "        \n",
    "        #Отличие здесь только в том, что с помощью кучи берем n самых встречаемых\n",
    "        if voc_size == None:\n",
    "            for sent in sents:\n",
    "                for word in sent:\n",
    "                    if self.frequencies[word] >= self.freq_threshold:\n",
    "                        self.word2idx[word] = idx\n",
    "                        self.idx2word[idx] = word\n",
    "                        idx += 1\n",
    "        else:\n",
    "            words_sorted_by_frequencies = heapq.nlargest(voc_size - len(self.idx2word), self.frequencies, key=self.frequencies.get)\n",
    "            for word in words_sorted_by_frequencies:\n",
    "                self.word2idx[word] = idx\n",
    "                self.idx2word[idx] = word\n",
    "                idx += 1\n",
    "    def rebuild_vocabulary(self, voc_size):\n",
    "        #На случай если нужно поменять размер словаря\n",
    "        self.idx2word = {0: '[PAD]', 1: '[CLS]', 2: '[SEP]', 3: '[UNK]'}\n",
    "        self.word2idx = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[UNK]': 3}\n",
    "        idx = len(self.word2idx)\n",
    "        words_sorted_by_frequencies = heapq.nlargest(voc_size - len(self.idx2word), self.frequencies, key=self.frequencies.get)\n",
    "        for word in words_sorted_by_frequencies:\n",
    "            self.word2idx[word] = idx\n",
    "            self.idx2word[idx] = word\n",
    "            idx += 1\n",
    "    def numericalize(self, tokens):\n",
    "        return [self.word2idx[token] if token in self.word2idx else self.word2idx['[UNK]']\n",
    "                for token in tokens]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим класс dataset для данных из nerus. Все документы пердзагрузим в токенезироанныом виде, каждому токену сопостаим его тег. По номеру элемента датасета мы должны уметь выдавать предложенитя в приведеном виде, так чтобы все элементы имели одну длину, если нужно обрезать их до одной длины, если нужно их дополнить [PAD], что необхдимо для объединения их в батчи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "class Nerus(Dataset):\n",
    "    def __init__(self, tokenizer,  n_docs=1000, filename=NEURUS, freq_threshold=1, pad_to_max_length = False, max_length = 20, voc_size = None):\n",
    "        docs_generator = load_nerus(filename)\n",
    "        docs = [next(docs_generator) for _ in range(n_docs)]\n",
    "        # array of splitted sentences for each document from docs\n",
    "        self.sents = []\n",
    "        self.sents_poses = []\n",
    "        for doc in docs:\n",
    "            for sent in doc.sents:\n",
    "                sents = []\n",
    "                poses = []\n",
    "                for token in sent.tokens:\n",
    "                    sents+=tokenizer.tokenize(token.text)\n",
    "                    \n",
    "                    t = ['[PAD]'] * len(tokenizer.tokenize(token.text))\n",
    "                    t[-1] = token.pos\n",
    "                    poses +=t\n",
    "                self.sents.append(sents)\n",
    "                self.sents_poses.append(poses)\n",
    "        \n",
    "        # initialize vocabularies and build them\n",
    "        self.words_vocab = WordsVocabulary(freq_threshold)\n",
    "        self.words_vocab.build_vocabulary(self.sents, voc_size = voc_size)\n",
    "        self.poses_vocab = PosesVocabulary()\n",
    "        self.poses_vocab.build_vocabulary()\n",
    "        self.pad_to_max_length =  pad_to_max_length\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sents)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.sents[idx]\n",
    "        poses = self.sents_poses[idx]\n",
    "        # tokens\n",
    "        max_length = self.max_length\n",
    "        if not self.pad_to_max_length:\n",
    "            max_length = min(max_length, len(tokens))\n",
    "        if len(self.words_vocab.numericalize(tokens)) < max_length :\n",
    "            numericalized_tokens = [self.words_vocab.word2idx['[CLS]']]+ self.words_vocab.numericalize(tokens)+[self.words_vocab.word2idx['[SEP]']] + [self.words_vocab.word2idx['[PAD]']]*(max_length-len(tokens))\n",
    "            numericalized_poses = [self.poses_vocab.pos2idx['[CLS]']]+ self.poses_vocab.numericalize(poses)+[self.poses_vocab.pos2idx['[SEP]']] + [self.poses_vocab.pos2idx['[PAD]']]*(max_length-len(tokens))          \n",
    "        else :\n",
    "            numericalized_tokens = [self.words_vocab.word2idx['[CLS]']]+ self.words_vocab.numericalize(tokens)[:max_length]+[self.words_vocab.word2idx['[SEP]']]\n",
    "            numericalized_poses = [self.poses_vocab.pos2idx['[CLS]']]+ self.poses_vocab.numericalize(poses)[:max_length]+[self.poses_vocab.pos2idx['[SEP]']] \n",
    "        source = torch.tensor(numericalized_tokens)\n",
    "        target = torch.tensor(numericalized_poses)\n",
    "        \n",
    "        return source, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенайзер ипользуем предложенный в задании LaBSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/LaBSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инициализируем датасет в приведеном виде"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Nerus(tokenizer, n_docs = 1000, max_length=100, pad_to_max_length=True, voc_size = 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на полученный размер словаря"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "303641"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.words_vocab.idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стандартные функции для обучения, примерно те же что и впредыдущем задании. Т.к. у нас все та же задача классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_batch(model, x_batch, y_batch, optimizer, loss_function):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output = model(x_batch.to(model.device))\n",
    "    num_class = output.shape[-1]\n",
    "    loss = loss_function(output.reshape(-1, num_class), y_batch.to(model.device).reshape(-1))\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    return loss.cpu().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_epoch(train_generator, model, loss_function, optimizer, callback = None):\n",
    "    epoch_loss = 0\n",
    "    total = 0\n",
    "    for it, (batch_of_x, batch_of_y) in enumerate(train_generator):\n",
    "        batch_loss = train_on_batch(model, batch_of_x, batch_of_y, optimizer, loss_function)\n",
    "        \n",
    "        if callback is not None:\n",
    "            with torch.no_grad():\n",
    "                callback(model, batch_loss)\n",
    "            \n",
    "        epoch_loss += batch_loss*len(batch_of_x)\n",
    "        total += len(batch_of_x)\n",
    "    \n",
    "    return epoch_loss/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(count_of_epoch, \n",
    "            batch_size, \n",
    "            dataset,\n",
    "            model, \n",
    "            loss_function,\n",
    "            optimizer,\n",
    "            lr = 0.001,\n",
    "            callback = None):\n",
    "\n",
    "    optima = optimizer(model.parameters(), lr=lr)\n",
    "    \n",
    "    iterations = tqdm.tqdm(range(count_of_epoch), desc='epoch')\n",
    "    iterations.set_postfix({'train epoch loss': np.nan})\n",
    "    epoch_loss = None\n",
    "    for it in iterations:\n",
    "        batch_generator = tqdm.tqdm(\n",
    "            torch.utils.data.DataLoader(dataset=dataset, \n",
    "                                        batch_size=batch_size, \n",
    "                                        shuffle=True, pin_memory=True), \n",
    "            leave=False, total=len(dataset)//batch_size+(len(dataset)%batch_size>0), position=1)\n",
    "        \n",
    "        epoch_loss = train_epoch(train_generator=batch_generator, \n",
    "                    model=model, \n",
    "                    loss_function=loss_function, \n",
    "                    optimizer=optima, \n",
    "                    callback=callback)\n",
    "        \n",
    "        iterations.set_postfix({'train epoch loss': epoch_loss})\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим какие токены имеют отдельные слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(['man',  'mankind','человек', 'сова', 'глаза', 'lamp','лампа','лемма', 'страна', 'вице-премьер', 'экспропреированный', 'телевезионный'], padding=True,\n",
    "                    truncation=True, \n",
    "                    max_length=512, \n",
    "                    return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   101,  15351,    102,      0,      0,      0,      0],\n",
       "        [   101, 415562,    102,      0,      0,      0,      0],\n",
       "        [   101,  23047,    102,      0,      0,      0,      0],\n",
       "        [   101,  15373,  16522,    102,      0,      0,      0],\n",
       "        [   101,  64861,    102,      0,      0,      0,      0],\n",
       "        [   101,  68097,    102,      0,      0,      0,      0],\n",
       "        [   101, 238225,    102,      0,      0,      0,      0],\n",
       "        [   101,  86379, 293807,    102,      0,      0,      0],\n",
       "        [   101,  20563,    102,      0,      0,      0,      0],\n",
       "        [   101,  74795,    118,  58084,    102,      0,      0],\n",
       "        [   101,  88753, 500786, 396721, 346325,  16846,    102],\n",
       "        [   101,  19102,  25964,  24931, 280944,    102,      0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 0]])}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Небольшое предложение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens1 = tokenizer(['Это телевезионный'], padding=True,\n",
    "                    truncation=True, \n",
    "                    max_length=512, \n",
    "                    return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 1, 1, 1, None]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens1.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'Это', 'теле', '##ве', '##зи', '##онный', '[SEP]']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens1.tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как это соотносится с их номерами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[88753, 500786, 396721, 346325, 16846]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(tokens1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   101,  15351,    102,      0,      0,      0,      0],\n",
       "        [   101, 415562,    102,      0,      0,      0,      0],\n",
       "        [   101,  23047,    102,      0,      0,      0,      0],\n",
       "        [   101,  15373,  16522,    102,      0,      0,      0],\n",
       "        [   101,  64861,    102,      0,      0,      0,      0],\n",
       "        [   101,  68097,    102,      0,      0,      0,      0],\n",
       "        [   101, 238225,    102,      0,      0,      0,      0],\n",
       "        [   101,  86379, 293807,    102,      0,      0,      0],\n",
       "        [   101,  20563,    102,      0,      0,      0,      0],\n",
       "        [   101,  74795,    118,  58084,    102,      0,      0],\n",
       "        [   101,  88753, 500786, 396721, 346325,  16846,    102],\n",
       "        [   101,  19102,  25964,  24931, 280944,    102,      0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 0]])}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качетстве классификатора использкем LSTM, которому еа входе подают эмбеддинги токенов, а ее выход подается в один полносвязанный слой. Выход сети представлется из себя вероятности тегов для токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RNNclassifier(torch.nn.Module):\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "    def __init__(self, vocab_dim, output_dim, emb_dim = 10, hidden_dim = 10, \n",
    "                 num_layers = 3, bidirectional = False, p=0.7, batch_norm = False):\n",
    "        super(RNNclassifier, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_dim, emb_dim)\n",
    "        self.encoder = torch.nn.LSTM(emb_dim, hidden_dim, num_layers, \n",
    "                                     bidirectional=bidirectional, \n",
    "                                     batch_first=True, dropout=p)\n",
    "        self.batch_norm = batch_norm\n",
    "        if (self.batch_norm): \n",
    "            self.bn = nn.BatchNorm1d(int(bidirectional + 1)*hidden_dim)\n",
    "        self.linear = torch.nn.Linear(\n",
    "            int(bidirectional + 1)*hidden_dim, \n",
    "            output_dim)\n",
    "        self.logsoftmax = nn.LogSoftmax()\n",
    "    def forward(self, input):\n",
    "        input = self.embedding(input)\n",
    "        lstm_out, _ = self.encoder(input)\n",
    "\n",
    "        if (self.batch_norm): \n",
    "            lstm_out = torch.transpose(lstm_out, -2, -1)\n",
    "            lstm_out = self.bn(lstm_out)\n",
    "            lstm_out = torch.transpose(lstm_out, -2, -1)\n",
    "        act = self.linear(lstm_out)\n",
    "        return act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(object):\n",
    "    def __init__(self, word_to_ind, tokenizer):\n",
    "        self.word_to_ind = word_to_ind\n",
    "        self.tokenizer = tokenizer\n",
    "    def __call__(self, sentences, max_length = 10, pad_to_max_length = False):\n",
    "        tokens = self.tokenizer.tokenize_sents(sentences)\n",
    "        if not pad_to_max_length:\n",
    "            max_length = min(max_length, max(map(len, tokens)))\n",
    "        tokens = [['[CLS]']+s+['[SEP]'] + ['[PAD]']*(max_length-len(s)) \\\n",
    "                  if len(s) < max_length \\\n",
    "                  else ['[CLS]']+s[:max_length]+['[SEP]'] \\\n",
    "                  for s in tokens ]\n",
    "        ids = [[self.word_to_ind.get(w, self.word_to_ind['[UNK]']) for w in sent] for sent in tokens]\n",
    "        return torch.tensor(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем на GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала иницализурем сет ьна случано подобранных гиперпараметрах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict()\n",
    "config['vocab_dim'] = len(dataset.words_vocab.word2idx)\n",
    "config['output_dim'] = len(dataset.poses_vocab.pos2idx)\n",
    "config['emb_dim'] = 100\n",
    "config['hidden_dim'] = 30\n",
    "config['num_layers'] = 4\n",
    "config['bidirectional'] = False\n",
    "config['p'] = 0.7\n",
    "config['batch_norm'] = True\n",
    "\n",
    "model = RNNclassifier(**config)\n",
    "_ = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['output_dim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.words_vocab.word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверять результаты классифкации будем на Recall, Accuracy и их среднем гармоническом F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   1, 5659,   12,  603,   19, 9033,  100, 2997, 2813, 1409, 5660,   17,\n",
       "         831,    4,    6, 2638, 1840,   44,   18, 1656, 1657, 3648, 2127, 9034,\n",
       "         904,  856, 1464,   23, 7025,    4,   62,  475,  495,    5,    2,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([102])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[117], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "dataset[0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помотрим как справлется с классификацией на датасете необученная сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sasha/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/sasha/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00  952216.0\n",
      "           1       0.00      0.00      0.00   11645.0\n",
      "           2       0.00      0.00      0.00   11645.0\n",
      "           3       0.00      0.00      0.00   19021.0\n",
      "           4       0.00      0.00      0.00    5405.0\n",
      "           5       0.00      0.00      0.00      12.0\n",
      "           6       0.00      0.00      0.00   58047.0\n",
      "           7       0.00      0.00      0.00   15256.0\n",
      "           8       0.00      0.00      0.00   24394.0\n",
      "           9       0.02      1.00      0.04   24138.0\n",
      "          10       0.00      0.00      0.00    1415.0\n",
      "          11       0.00      0.00      0.00    5005.0\n",
      "          12       0.00      0.00      0.00    3271.0\n",
      "          13       0.00      0.00      0.00    3987.0\n",
      "          14       0.00      0.00      0.00    2537.0\n",
      "          15       0.00      0.00      0.00    7244.0\n",
      "          16       0.00      0.00      0.00    3608.0\n",
      "          17       0.00      0.00      0.00   37007.0\n",
      "          18       0.00      0.00      0.00      48.0\n",
      "          19       0.00      0.00      0.00    1889.0\n",
      "\n",
      "    accuracy                           0.02 1187790.0\n",
      "   macro avg       0.00      0.05      0.00 1187790.0\n",
      "weighted avg       0.00      0.02      0.00 1187790.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sasha/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "batch_generator = torch.utils.data.DataLoader(dataset=dataset, \n",
    "                                              batch_size=64, \n",
    "                                              pin_memory=True)\n",
    "            \n",
    "pred = []\n",
    "real = []\n",
    "model.eval()\n",
    "for its, (x_batch, y_batch) in enumerate(batch_generator):\n",
    "    x_batch = x_batch.to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(x_batch)\n",
    "    pred.extend(torch.argmax(output, dim=-1).cpu().numpy().flatten().tolist())\n",
    "    real.extend(y_batch.cpu().numpy().flatten().tolist())\n",
    "real = np.array(real)\n",
    "pred = np.array(pred)\n",
    "print(classification_report(real, pred, sample_weight= 1.0 * (real != 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично предыдущему заданию исаользем в качестве функции потреь кросс-энтропию, а в качестве оптимизатора Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на качество обучения на этих значениях гиперпараметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 20/20 [00:22<00:00,  1.15s/it, train epoch loss=0.525]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5246788402952929"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer(count_of_epoch=20, \n",
    "        batch_size=64, \n",
    "        dataset=dataset,\n",
    "        model=model, \n",
    "        loss_function=loss_function,\n",
    "        optimizer = optimizer,\n",
    "        lr=0.001,\n",
    "        callback=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sasha/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sasha/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00  952216.0\n",
      "           1       1.00      1.00      1.00   11645.0\n",
      "           2       0.49      0.99      0.66   11645.0\n",
      "           3       0.07      0.12      0.09   19021.0\n",
      "           4       0.47      0.12      0.20    5405.0\n",
      "           5       0.00      0.00      0.00      12.0\n",
      "           6       0.39      0.97      0.55   58047.0\n",
      "           7       0.09      0.03      0.04   15256.0\n",
      "           8       0.34      0.76      0.47   24394.0\n",
      "           9       0.92      0.12      0.22   24138.0\n",
      "          10       0.00      0.00      0.00    1415.0\n",
      "          11       0.64      0.57      0.60    5005.0\n",
      "          12       0.00      0.00      0.00    3271.0\n",
      "          13       0.00      0.00      0.00    3987.0\n",
      "          14       0.33      0.05      0.09    2537.0\n",
      "          15       0.12      0.51      0.19    7244.0\n",
      "          16       0.39      0.84      0.53    3608.0\n",
      "          17       0.04      1.00      0.08   37007.0\n",
      "          18       0.00      0.00      0.00      48.0\n",
      "          19       0.00      0.00      0.00    1889.0\n",
      "\n",
      "    accuracy                           0.13 1187790.0\n",
      "   macro avg       0.26      0.35      0.24 1187790.0\n",
      "weighted avg       0.07      0.13      0.07 1187790.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sasha/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "batch_generator = torch.utils.data.DataLoader(dataset=dataset, \n",
    "                                              batch_size=64, \n",
    "                                              pin_memory=True)\n",
    "            \n",
    "pred = []\n",
    "real = []\n",
    "model.eval()\n",
    "for its, (x_batch, y_batch) in enumerate(batch_generator):\n",
    "    x_batch = x_batch.to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(x_batch)\n",
    "    pred.extend(torch.argmax(output, dim=-1).cpu().numpy().flatten().tolist())\n",
    "    real.extend(y_batch.cpu().numpy().flatten().tolist())\n",
    "real = np.array(real)\n",
    "pred = np.array(pred)\n",
    "print(classification_report(real, pred, sample_weight= 1.0 * (real != 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим для некторых классов повысилась не только точность угадываения Accuracy, но и доля угадыннх меток относительно всего класса Recall. Но вот с более редко встречающимися классами проблема, для них резултаты не поменялись и модель их просто не выдает."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 6, 6, ..., 6, 6, 6])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем перебрать структуру самой сети с помощью optuna как в прошлом задании."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-03 04:52:51.096005: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Опять логгирование при обучении вынесем в отдельный интерфейс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class callback():\n",
    "    def __init__(self, writer, dataset, loss_function, metrics, hparams,  delimeter = 100, batch_size=64):\n",
    "        self.step = 0\n",
    "        self.writer = writer\n",
    "        self.delimeter = delimeter\n",
    "        self.loss_function = loss_function\n",
    "        self.batch_size = batch_size\n",
    "        self.hparams = hparams\n",
    "        self.dataset = dataset\n",
    "        self.metrics = metrics\n",
    "\n",
    "    def forward(self, model, loss):\n",
    "        self.step += 1\n",
    "        with self.writer.as_default():\n",
    "            hp.hparams(self.hparams)\n",
    "            tf.summary.scalar('Loss', loss, self.step)\n",
    "        \n",
    "        if self.step % self.delimeter == 0:\n",
    "            \n",
    "            batch_generator = torch.utils.data.DataLoader(dataset = self.dataset, \n",
    "                                                          batch_size=self.batch_size)\n",
    "            \n",
    "            pred = []\n",
    "            real = []\n",
    "            test_loss = 0\n",
    "            model.eval()\n",
    "            \n",
    "            for its, (x_batch, y_batch) in enumerate(batch_generator):\n",
    "                x_batch = x_batch.to(device)\n",
    "                with torch.no_grad():\n",
    "                    output = model(x_batch)\n",
    "                pred.extend(torch.argmax(output, dim=-1).cpu().numpy().flatten().tolist())\n",
    "                real.extend(y_batch.cpu().numpy().flatten().tolist())\n",
    "            real = np.array(real)\n",
    "            pred = np.array(pred)\n",
    "\n",
    "            test_loss = self.loss_function(real, pred)\n",
    "            with self.writer.as_default():\n",
    "                tf.summary.scalar(self.metrics, test_loss, self.step)\n",
    "\n",
    "            x = x_batch[-10:]\n",
    "          \n",
    "    def __call__(self, model, loss):\n",
    "        return self.forward(model, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import optuna_dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим сетку на которой будем перебирать гиперпараметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_VOCAB_DIM = hp.HParam('vocab_dim', hp.IntInterval(1* 1000, 10* 1000))\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.5, 0.8))\n",
    "HP_BATCH_NORM = hp.HParam('batch_norm', hp.Discrete([True, False]))\n",
    "HP_HIDDEN_DIM = hp.HParam('hidden_dim', hp.IntInterval(5, 30))\n",
    "HP_EMB_DIM = hp.HParam('emb_dim', hp.IntInterval(50, 100))\n",
    "HP_NUM_LAYERS = hp.HParam('num_layers', hp.IntInterval(1, 4))\n",
    "METRICS_NAME = \"Accuracy\"\n",
    "DELIMETER = 100\n",
    "IMAGE_SIZE = 28\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE  = 64\n",
    "COLORS = 1\n",
    "MIN_CHANNELS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждый набор гиперпараметром логируем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-03 04:52:57.684607: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-03 04:52:57.695076: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-03 04:52:57.695111: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-03 04:52:57.697117: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-03 04:52:57.697141: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-03 04:52:57.697155: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-03 04:52:57.840929: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-03 04:52:57.840980: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-03 04:52:57.840988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1722] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-04-03 04:52:57.841005: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-03 04:52:57.841033: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7537 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:2d:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "logs_dir = 'logs/hparam_tuning'\n",
    "writer_hparam = tf.summary.create_file_writer('logs/hparam_tuning')\n",
    "with writer_hparam.as_default():\n",
    "  hp.hparams_config(\n",
    "    hparams=[HP_VOCAB_DIM,\n",
    "      HP_EMB_DIM,\n",
    "      HP_HIDDEN_DIM,\n",
    "      HP_NUM_LAYERS,\n",
    "      HP_BATCH_NORM,\n",
    "      HP_DROPOUT],\n",
    "    metrics=[hp.Metric(METRICS_NAME , display_name='Accuracy')],\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самая простая метрика которую можн овзять - это точность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_accuracy(pred, real):\n",
    "    return sklearn.metrics.accuracy_score(real, pred, sample_weight= 1.0 * (real != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 40153), started 0:00:03 ago. (Use '!kill 40153' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-23b3ad085ad37053\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-23b3ad085ad37053\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/hparam_tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим перебор гиперпараметров с помощью optina, обернув это в функцию, которая по параметрам выдает значение метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-03 04:53:10,962] A new study created in memory with name: no-name-30b81933-83e1-4bf9-b28a-b9aea9726222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current run: logs/hparam_tuning/run-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 100/100 [04:05<00:00,  2.46s/it, train epoch loss=0.828]\n",
      "[I 2024-04-03 04:57:16,692] Trial 0 finished with value: 0.828410155464007 and parameters: {'vocab_dim': 6, 'dropout': 0.6916383320313331, 'emb_dim': 97, 'hidden_dim': 15, 'batch_norm': False, 'layers_count': 3}. Best is trial 0 with value: 0.828410155464007.\n",
      "/home/sasha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.7539793858558304 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.828410155464007\n",
      "Current run: logs/hparam_tuning/run-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 100/100 [03:51<00:00,  2.32s/it, train epoch loss=0.241]\n",
      "[I 2024-04-03 05:01:08,573] Trial 1 finished with value: 0.2414513362175715 and parameters: {'vocab_dim': 3, 'dropout': 0.7539793858558304, 'emb_dim': 72, 'hidden_dim': 15, 'batch_norm': False, 'layers_count': 1}. Best is trial 0 with value: 0.828410155464007.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2414513362175715\n",
      "Current run: logs/hparam_tuning/run-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 100/100 [04:13<00:00,  2.53s/it, train epoch loss=0.435]\n",
      "[I 2024-04-03 05:05:22,075] Trial 2 finished with value: 0.4352338940199479 and parameters: {'vocab_dim': 10, 'dropout': 0.5858516834162082, 'emb_dim': 89, 'hidden_dim': 29, 'batch_norm': False, 'layers_count': 4}. Best is trial 0 with value: 0.828410155464007.\n",
      "/home/sasha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6156701810242755 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4352338940199479\n",
      "Current run: logs/hparam_tuning/run-3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 100/100 [03:49<00:00,  2.29s/it, train epoch loss=0.515]\n",
      "[I 2024-04-03 05:09:11,420] Trial 3 finished with value: 0.5149787447748393 and parameters: {'vocab_dim': 1, 'dropout': 0.6156701810242755, 'emb_dim': 95, 'hidden_dim': 9, 'batch_norm': False, 'layers_count': 1}. Best is trial 0 with value: 0.828410155464007.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5149787447748393\n",
      "Current run: logs/hparam_tuning/run-4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 100/100 [04:08<00:00,  2.49s/it, train epoch loss=0.587]\n",
      "[I 2024-04-03 05:13:20,196] Trial 4 finished with value: 0.5870874889524467 and parameters: {'vocab_dim': 6, 'dropout': 0.5009906004958044, 'emb_dim': 93, 'hidden_dim': 12, 'batch_norm': False, 'layers_count': 3}. Best is trial 0 with value: 0.828410155464007.\n",
      "/home/sasha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5543456065149572 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5870874889524467\n",
      "Current run: logs/hparam_tuning/run-5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 100/100 [04:02<00:00,  2.43s/it, train epoch loss=0.0159]\n",
      "[I 2024-04-03 05:17:22,749] Trial 5 finished with value: 0.015899894417800608 and parameters: {'vocab_dim': 8, 'dropout': 0.5543456065149572, 'emb_dim': 65, 'hidden_dim': 25, 'batch_norm': True, 'layers_count': 1}. Best is trial 0 with value: 0.828410155464007.\n",
      "/home/sasha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6541551157409462 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015899894417800608\n",
      "Current run: logs/hparam_tuning/run-6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 100/100 [04:16<00:00,  2.57s/it, train epoch loss=0.0706]\n",
      "[I 2024-04-03 05:21:39,720] Trial 6 finished with value: 0.07062117609664471 and parameters: {'vocab_dim': 4, 'dropout': 0.6541551157409462, 'emb_dim': 95, 'hidden_dim': 25, 'batch_norm': True, 'layers_count': 1}. Best is trial 0 with value: 0.828410155464007.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07062117609664471\n",
      "Current run: logs/hparam_tuning/run-7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 100/100 [04:05<00:00,  2.46s/it, train epoch loss=0.983]\n",
      "[I 2024-04-03 05:25:45,657] Trial 7 finished with value: 0.9825970407922465 and parameters: {'vocab_dim': 4, 'dropout': 0.6695602738922346, 'emb_dim': 100, 'hidden_dim': 9, 'batch_norm': True, 'layers_count': 4}. Best is trial 7 with value: 0.9825970407922465.\n",
      "/home/sasha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5927551164548851 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9825970407922465\n",
      "Current run: logs/hparam_tuning/run-8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 100/100 [04:12<00:00,  2.53s/it, train epoch loss=0.0161]\n",
      "[I 2024-04-03 05:29:58,593] Trial 8 finished with value: 0.0160606201100709 and parameters: {'vocab_dim': 9, 'dropout': 0.5927551164548851, 'emb_dim': 69, 'hidden_dim': 18, 'batch_norm': True, 'layers_count': 1}. Best is trial 7 with value: 0.9825970407922465.\n",
      "/home/sasha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.7012781375577928 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0160606201100709\n",
      "Current run: logs/hparam_tuning/run-9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 100/100 [04:10<00:00,  2.51s/it, train epoch loss=0.255]\n",
      "[I 2024-04-03 05:34:09,539] Trial 9 finished with value: 0.2551578123920616 and parameters: {'vocab_dim': 3, 'dropout': 0.7012781375577928, 'emb_dim': 55, 'hidden_dim': 9, 'batch_norm': True, 'layers_count': 1}. Best is trial 7 with value: 0.9825970407922465.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2551578123920616\n",
      "Current run: logs/hparam_tuning/run-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 100/100 [04:07<00:00,  2.47s/it, train epoch loss=1.83]\n",
      "[I 2024-04-03 05:38:16,953] Trial 10 finished with value: 1.8275855116498168 and parameters: {'vocab_dim': 1, 'dropout': 0.7763459654879962, 'emb_dim': 83, 'hidden_dim': 6, 'batch_norm': True, 'layers_count': 4}. Best is trial 10 with value: 1.8275855116498168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8275855116498168\n",
      "Current run: logs/hparam_tuning/run-11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 100/100 [04:04<00:00,  2.44s/it, train epoch loss=1.85]\n",
      "[I 2024-04-03 05:42:21,286] Trial 11 finished with value: 1.8487955672078606 and parameters: {'vocab_dim': 1, 'dropout': 0.7991440144916659, 'emb_dim': 83, 'hidden_dim': 5, 'batch_norm': True, 'layers_count': 4}. Best is trial 11 with value: 1.8487955672078606.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8487955672078606\n",
      "Current run: logs/hparam_tuning/run-12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 100/100 [04:06<00:00,  2.46s/it, train epoch loss=1.87]\n",
      "[I 2024-04-03 05:46:27,643] Trial 12 finished with value: 1.8662073695234804 and parameters: {'vocab_dim': 1, 'dropout': 0.7953055750286426, 'emb_dim': 82, 'hidden_dim': 5, 'batch_norm': True, 'layers_count': 4}. Best is trial 12 with value: 1.8662073695234804.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8662073695234804\n",
      "Current run: logs/hparam_tuning/run-13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 100/100 [04:02<00:00,  2.42s/it, train epoch loss=1.7]\n",
      "[I 2024-04-03 05:50:30,007] Trial 13 finished with value: 1.704812552540944 and parameters: {'vocab_dim': 1, 'dropout': 0.7953682077702475, 'emb_dim': 81, 'hidden_dim': 5, 'batch_norm': True, 'layers_count': 3}. Best is trial 12 with value: 1.8662073695234804.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.704812552540944\n",
      "Current run: logs/hparam_tuning/run-14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 100/100 [04:06<00:00,  2.46s/it, train epoch loss=1.82]\n",
      "[I 2024-04-03 05:54:36,073] Trial 14 finished with value: 1.8230235767446397 and parameters: {'vocab_dim': 2, 'dropout': 0.736559761377283, 'emb_dim': 80, 'hidden_dim': 5, 'batch_norm': True, 'layers_count': 4}. Best is trial 12 with value: 1.8662073695234804.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8230235767446397\n",
      "Current run: logs/hparam_tuning/run-15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 100/100 [04:00<00:00,  2.40s/it, train epoch loss=0.368]\n",
      "[I 2024-04-03 05:58:36,399] Trial 15 finished with value: 0.3679536295658839 and parameters: {'vocab_dim': 4, 'dropout': 0.718517046633522, 'emb_dim': 85, 'hidden_dim': 19, 'batch_norm': True, 'layers_count': 2}. Best is trial 12 with value: 1.8662073695234804.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3679536295658839\n",
      "Current run: logs/hparam_tuning/run-16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 100/100 [03:58<00:00,  2.39s/it, train epoch loss=0.749]\n",
      "[I 2024-04-03 06:02:35,056] Trial 16 finished with value: 0.7491966027412972 and parameters: {'vocab_dim': 2, 'dropout': 0.7958094874398419, 'emb_dim': 64, 'hidden_dim': 13, 'batch_norm': True, 'layers_count': 2}. Best is trial 12 with value: 1.8662073695234804.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7491966027412972\n",
      "Current run: logs/hparam_tuning/run-17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 100/100 [04:12<00:00,  2.53s/it, train epoch loss=0.476]\n",
      "[I 2024-04-03 06:06:47,652] Trial 17 finished with value: 0.47639568684284345 and parameters: {'vocab_dim': 7, 'dropout': 0.7484565631313602, 'emb_dim': 77, 'hidden_dim': 21, 'batch_norm': True, 'layers_count': 3}. Best is trial 12 with value: 1.8662073695234804.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47639568684284345\n",
      "Current run: logs/hparam_tuning/run-18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 100/100 [04:06<00:00,  2.47s/it, train epoch loss=1.45]\n",
      "[I 2024-04-03 06:10:54,476] Trial 18 finished with value: 1.4514003936391677 and parameters: {'vocab_dim': 2, 'dropout': 0.7658392965756634, 'emb_dim': 88, 'hidden_dim': 8, 'batch_norm': True, 'layers_count': 4}. Best is trial 12 with value: 1.8662073695234804.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4514003936391677\n",
      "Current run: logs/hparam_tuning/run-19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 100/100 [04:09<00:00,  2.50s/it, train epoch loss=0.896]\n",
      "[I 2024-04-03 06:15:04,363] Trial 19 finished with value: 0.8957599593109714 and parameters: {'vocab_dim': 5, 'dropout': 0.7201973739835417, 'emb_dim': 75, 'hidden_dim': 12, 'batch_norm': True, 'layers_count': 4}. Best is trial 12 with value: 1.8662073695234804.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8957599593109714\n",
      "Current run: logs/hparam_tuning/run-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 100/100 [03:59<00:00,  2.40s/it, train epoch loss=0.724]\n",
      "[I 2024-04-03 06:19:04,341] Trial 20 finished with value: 0.7243506022766968 and parameters: {'vocab_dim': 3, 'dropout': 0.6312289664110131, 'emb_dim': 50, 'hidden_dim': 7, 'batch_norm': True, 'layers_count': 2}. Best is trial 12 with value: 1.8662073695234804.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7243506022766968\n",
      "Current run: logs/hparam_tuning/run-21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 100/100 [04:06<00:00,  2.46s/it, train epoch loss=1.72]\n",
      "[I 2024-04-03 06:23:10,470] Trial 21 finished with value: 1.7196658512511034 and parameters: {'vocab_dim': 1, 'dropout': 0.7822164854634648, 'emb_dim': 84, 'hidden_dim': 6, 'batch_norm': True, 'layers_count': 4}. Best is trial 12 with value: 1.8662073695234804.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7196658512511034\n",
      "Current run: logs/hparam_tuning/run-22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 100/100 [04:04<00:00,  2.45s/it, train epoch loss=1.89]\n",
      "[I 2024-04-03 06:27:15,240] Trial 22 finished with value: 1.8945633379775293 and parameters: {'vocab_dim': 1, 'dropout': 0.7748024153402618, 'emb_dim': 89, 'hidden_dim': 5, 'batch_norm': True, 'layers_count': 4}. Best is trial 22 with value: 1.8945633379775293.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8945633379775293\n",
      "Current run: logs/hparam_tuning/run-23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 100/100 [04:02<00:00,  2.43s/it, train epoch loss=1.22]\n",
      "[I 2024-04-03 06:31:17,876] Trial 23 finished with value: 1.222402932225199 and parameters: {'vocab_dim': 2, 'dropout': 0.7978478576158479, 'emb_dim': 91, 'hidden_dim': 10, 'batch_norm': True, 'layers_count': 3}. Best is trial 22 with value: 1.8945633379775293.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.222402932225199\n",
      "Current run: logs/hparam_tuning/run-24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 100/100 [04:03<00:00,  2.44s/it, train epoch loss=1.8]\n",
      "[I 2024-04-03 06:35:21,798] Trial 24 finished with value: 1.7952855259320828 and parameters: {'vocab_dim': 1, 'dropout': 0.7663393497591658, 'emb_dim': 87, 'hidden_dim': 5, 'batch_norm': True, 'layers_count': 4}. Best is trial 22 with value: 1.8945633379775293.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7952855259320828\n",
      "Current run: logs/hparam_tuning/run-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 100/100 [04:04<00:00,  2.45s/it, train epoch loss=1]  \n",
      "[I 2024-04-03 06:39:26,360] Trial 25 finished with value: 1.004807043392907 and parameters: {'vocab_dim': 3, 'dropout': 0.7352840920990291, 'emb_dim': 72, 'hidden_dim': 11, 'batch_norm': True, 'layers_count': 3}. Best is trial 22 with value: 1.8945633379775293.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.004807043392907\n",
      "Current run: logs/hparam_tuning/run-26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 100/100 [04:04<00:00,  2.44s/it, train epoch loss=1.59]\n",
      "[I 2024-04-03 06:43:30,649] Trial 26 finished with value: 1.5909253212759658 and parameters: {'vocab_dim': 2, 'dropout': 0.7701997923456856, 'emb_dim': 78, 'hidden_dim': 7, 'batch_norm': False, 'layers_count': 4}. Best is trial 22 with value: 1.8945633379775293.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5909253212759658\n",
      "Current run: logs/hparam_tuning/run-27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 100/100 [04:09<00:00,  2.49s/it, train epoch loss=0.771]\n",
      "[I 2024-04-03 06:47:40,111] Trial 27 finished with value: 0.7714528085899844 and parameters: {'vocab_dim': 5, 'dropout': 0.6923369130355432, 'emb_dim': 90, 'hidden_dim': 14, 'batch_norm': True, 'layers_count': 4}. Best is trial 22 with value: 1.8945633379775293.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7714528085899844\n",
      "Current run: logs/hparam_tuning/run-28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 100/100 [04:02<00:00,  2.43s/it, train epoch loss=1.42]\n",
      "[I 2024-04-03 06:51:42,870] Trial 28 finished with value: 1.4228852876116145 and parameters: {'vocab_dim': 1, 'dropout': 0.7999343620622313, 'emb_dim': 80, 'hidden_dim': 8, 'batch_norm': True, 'layers_count': 3}. Best is trial 22 with value: 1.8945633379775293.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4228852876116145\n",
      "Current run: logs/hparam_tuning/run-29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 100/100 [04:04<00:00,  2.45s/it, train epoch loss=1.26]\n",
      "[I 2024-04-03 06:55:47,647] Trial 29 finished with value: 1.2612346751744166 and parameters: {'vocab_dim': 2, 'dropout': 0.6725142185466407, 'emb_dim': 99, 'hidden_dim': 21, 'batch_norm': False, 'layers_count': 4}. Best is trial 22 with value: 1.8945633379775293.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2612346751744166\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = 30\n",
    "def objective(trial):\n",
    "    run_number = trial.number\n",
    "    run_name = logs_dir + f'/run-{run_number}'\n",
    "    print(\"Current run: \" + run_name)\n",
    "    vocab_dim =  trial.suggest_int('vocab_dim', 1, 10) * 1000\n",
    "    dropout = trial.suggest_float('dropout', 0.5, 0.8)\n",
    "    emb_dim = trial.suggest_int('emb_dim', 50, 100)\n",
    "    hidden_dim = trial.suggest_int('hidden_dim', 5, 30)\n",
    "    batch_norm = trial.suggest_categorical('batch_norm', [True, False])\n",
    "    num_layers = trial.suggest_int('layers_count', 1, 4)\n",
    "    writer = tf.summary.create_file_writer(run_name)\n",
    "\n",
    "    hparams = {\n",
    "      HP_VOCAB_DIM: vocab_dim,\n",
    "      HP_EMB_DIM:  emb_dim,\n",
    "      HP_HIDDEN_DIM: hidden_dim,\n",
    "      HP_NUM_LAYERS: num_layers,\n",
    "      HP_BATCH_NORM: batch_norm,\n",
    "      HP_DROPOUT: dropout\n",
    "    }\n",
    "    dataset.words_vocab.rebuild_vocabulary(hparams[HP_VOCAB_DIM])\n",
    "    config = dict()\n",
    "    config['vocab_dim'] = len(dataset.words_vocab.word2idx)\n",
    "    config['output_dim'] = len(dataset.poses_vocab.pos2idx)\n",
    "    config['emb_dim'] = hparams[HP_EMB_DIM]\n",
    "    config['hidden_dim'] = hparams[HP_HIDDEN_DIM]\n",
    "    config['num_layers'] = hparams[HP_NUM_LAYERS]\n",
    "    config['bidirectional'] = False\n",
    "    config['p'] = hparams[HP_DROPOUT]\n",
    "    config['batch_norm'] = hparams[HP_BATCH_NORM]\n",
    "    device = torch.device(\"cuda\")\n",
    "    rnn = RNNclassifier(**config)\n",
    "    rnn = rnn.to(device)\n",
    "    loss = trainer(count_of_epoch=EPOCHS, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        dataset=dataset,\n",
    "        model=rnn, \n",
    "        loss_function=torch.nn.CrossEntropyLoss(ignore_index=0),\n",
    "        optimizer = torch.optim.Adam,\n",
    "        lr=0.001,\n",
    "        callback=callback(writer, dataset, prob_accuracy, METRICS_NAME , hparams, DELIMETER))\n",
    "    \n",
    "    print(loss)\n",
    "    return loss\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=NUM_TRIALS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучшие значение гиперпараметров, полученные optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_dim': 1,\n",
       " 'dropout': 0.7748024153402618,\n",
       " 'emb_dim': 89,\n",
       " 'hidden_dim': 5,\n",
       " 'batch_norm': True,\n",
       " 'layers_count': 4}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучшая метрика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8945633379775293"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предварительные выводы по результатам перебора гиперпараметров\n",
    "Рассмотрим график полученный в tensorboard на hpparams - parralel coordinates view.\n",
    "Из него делается несколько выводов. \n",
    "\n",
    "Лучше себя показывет модель с небольшим по размеру слоем, зато с общем числом слоев $4$. \n",
    "\n",
    "Увеличение словаря ухудшило результаты обучения, лучше себя показал небольшой словарь, что довольно неожиданно, но возможно при увеличении размера словаря требуется соответсвенно увеличивать и сложность сети.\n",
    "\n",
    "Для предотвращения перобучения требуется использовать больший $\\text{dropout} = 0.7$.\n",
    "\n",
    "При увеличении словаря требуется увеличивать и $\\text{embedding}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем еще раз обучить с подобранными параметрами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict()\n",
    "dataset.words_vocab.rebuild_vocabulary(10000)\n",
    "config['vocab_dim'] = len(dataset.words_vocab.word2idx)\n",
    "config['output_dim'] = len(dataset.poses_vocab.pos2idx)\n",
    "config['emb_dim'] = 90\n",
    "config['hidden_dim'] = 30\n",
    "config['num_layers'] = 1\n",
    "config['bidirectional'] = False\n",
    "config['p'] = 0\n",
    "config['batch_norm'] = True\n",
    "device = torch.device(\"cuda\")\n",
    "rnn = RNNclassifier(**config)\n",
    "rnn = rnn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 20/20 [00:22<00:00,  1.12s/it, train epoch loss=0.068] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06796524720726324"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer(count_of_epoch=20, \n",
    "        batch_size=64, \n",
    "        dataset=dataset,\n",
    "        model=rnn, \n",
    "        loss_function=loss_function,\n",
    "        optimizer = optimizer,\n",
    "        lr=0.001,\n",
    "        callback=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sasha/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/sasha/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/sasha/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/sasha/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/sasha/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/sasha/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       0.0\n",
      "           1       1.00      1.00      1.00   11645.0\n",
      "           2       1.00      1.00      1.00   11645.0\n",
      "           3       0.92      0.96      0.94   19021.0\n",
      "           4       0.99      0.93      0.96    5405.0\n",
      "           5       0.80      0.33      0.47      12.0\n",
      "           6       0.98      0.97      0.98   58047.0\n",
      "           7       0.98      0.97      0.98   15256.0\n",
      "           8       0.97      0.98      0.97   24394.0\n",
      "           9       1.00      1.00      1.00   24138.0\n",
      "          10       0.95      0.97      0.96    1415.0\n",
      "          11       0.99      0.99      0.99    5005.0\n",
      "          12       0.93      0.91      0.92    3271.0\n",
      "          13       0.96      0.95      0.95    3987.0\n",
      "          14       0.98      0.97      0.97    2537.0\n",
      "          15       0.98      0.95      0.96    7244.0\n",
      "          16       0.95      0.99      0.97    3608.0\n",
      "          17       1.00      1.00      1.00   37007.0\n",
      "          18       1.00      0.73      0.84      48.0\n",
      "          19       0.88      0.92      0.90    1889.0\n",
      "\n",
      "    accuracy                           0.98  235574.0\n",
      "   macro avg       0.91      0.88      0.89  235574.0\n",
      "weighted avg       0.98      0.98      0.98  235574.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sasha/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/sasha/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/sasha/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "batch_generator = torch.utils.data.DataLoader(dataset=dataset, \n",
    "                                              batch_size=64, \n",
    "                                              pin_memory=True)\n",
    "            \n",
    "pred = []\n",
    "real = []\n",
    "rnn.eval()\n",
    "for its, (x_batch, y_batch) in enumerate(batch_generator):\n",
    "    x_batch = x_batch.to(device)\n",
    "    with torch.no_grad():\n",
    "        output = rnn(x_batch)\n",
    "    pred.extend(torch.argmax(output, dim=-1).cpu().numpy().flatten().tolist())\n",
    "    real.extend(y_batch.cpu().numpy().flatten().tolist())\n",
    "real = np.array(real)\n",
    "pred = np.array(pred)\n",
    "print(classification_report(real, pred, sample_weight= 1.0 * (real != 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат нам ного лучше после обучения на подобранных параметрах. Теперь теги правильно выдаются не только для самых популярных классов. Предыдущей модели обучится на них мешала недостаточная сложность можели, из-за которых она не могла подстроится под маленькие классы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим как модель пресказывает тэги для случайных предложений, а не из датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(object):\n",
    "    def __init__(self, word_to_ind, tokenizer):\n",
    "        self.word_to_ind = word_to_ind\n",
    "        self.tokenizer = tokenizer\n",
    "    def __call__(self, sentences, max_length = 100, pad_to_max_length = False):\n",
    "        tokens = self.tokenizer(sentences).tokens()\n",
    "        print(tokens)\n",
    "        if not pad_to_max_length:\n",
    "            max_length = min(max_length, max(map(len, tokens)))\n",
    "        tokens = tokens + ['[PAD]']*(max_length-len(tokens)) \\\n",
    "                  if len(tokens) < max_length \\\n",
    "                  else tokens\n",
    "        print(tokens)\n",
    "        ids = [self.word_to_ind.get(w, self.word_to_ind['[UNK]']) for w in tokens]\n",
    "        return torch.tensor(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем сами простое предложение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = \"Вполне простое предложение\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизируем его"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'В', '##пол', '##не', 'простое', 'предложение', '[SEP]']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(prob).tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = Tokenizer(dataset.words_vocab.word2idx, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, что мы не забыли добавить маркеры начала и конца "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'В', '##пол', '##не', 'простое', 'предложение', '[SEP]']\n",
      "['[CLS]', 'В', '##пол', '##не', 'простое', 'предложение', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "tokenized_prob = token(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   1,   16,  206,  325,    3, 3234,    2,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_prob = tokenized_prob.unsqueeze(dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNclassifier(\n",
       "  (embedding): Embedding(10000, 90)\n",
       "  (encoder): LSTM(90, 30, batch_first=True)\n",
       "  (bn): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (linear): Linear(in_features=30, out_features=20, bias=True)\n",
       "  (logsoftmax): LogSoftmax(dim=None)\n",
       ")"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenized_prob = tokenized_prob.to(rnn.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.eval()\n",
    "output = rnn(tokenized_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предскажем тэги с помощью модели, выберем намболее вероятныцй класс для каждого тега"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = torch.argmax(output, dim=-1).cpu().numpy().flatten().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 9, 12, 6, 8, 6, 2, 7, 3, 3, 6]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in pred:\n",
    "    res.append(dataset.poses_vocab.idx2pos[e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'ADP',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'NOUN',\n",
       " '[SEP]',\n",
       " 'PROPN',\n",
       " 'ADJ',\n",
       " 'ADJ',\n",
       " 'NOUN']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вывод\n",
    "Перебор гиперпараметров для модели очень важен. Если взять недостаточно простую модель, как мы взяли вначале это ей не позолит обучиться под классы, т.к. она недостаторчно гибкая. Слишком сожная модель переобучитсья. Нормализация позволяет не переобучиться также. Причем увеличение числа слоев помогало обучаться модели, в то же время увеличение размера скрытого слоя приводило наоборот к ухудшению метрики. Лучше всего показала себя модель с небольшими по размеру слоями, но при этом соостоящая из 4 слоев. При этом увеличение рамера словаря не вело к улучшению метрик, намного лучше себя показывали модели с небольшим словаряем."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
