{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Реализация алгоритма Advantage-Actor Critic (A2C) (вплоть до 10 баллов)\n",
    "\n",
    "#### дедлайн задания (сразу жёсткий): 26 апреля, 23:59 UTC+3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа выполнена: Терентьев Александр Андреевич, Б05-003."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной работе Вам предстоит реализовать алгоритм `Advantage Actor Critic`, обучаемый на батче из сред `Atari 2600`, работающих параллельно.\n",
    "\n",
    "Для начала будут использованы обёртки сред, реализованные в файле `atari_wrappers.py`. Эти обёртки предварительно обрабатывают наблюдения (производят преобразования размера, цвета фрейма, взятия максимума между фреймами, пропускают часть фреймов и сводят несколько фреймов в один большой) и вознаграждения. Некоторые обёртки помогают автоматически перезапустить среду и присвоить переменной `done` значение `True` в случае смерти агента. Файл `env_batch.py` включает в себя реализацию класса `ParallelEnvBatch`, позволяющего запускать несколько сред параллельно. Для создания (инициализации) среды можно воспользоваться функцией `nature_dqn_env`. Обратите внимание, что в случае использования `PyTorch` (https://pytorch.org/) без `tensorboardX` (https://github.com/lanpa/tensorboardX) потребуется самостоятельно реализовать обёртку среды, которая будет логрировать **исходные** суммарные награды, которые *исходная* среда возвращает, и переопределить реализацию функции `nature_dqn_env`. То есть настоятельно рекомендуется применить `tensorboardX`.\n",
    "\n",
    "Псевдокод алгоритма `Advantage Actor Critic (A2C)` приведён в **Разделе 5.2.5 (Алгоритм 20)** конспекта лекций: https://arxiv.org/pdf/2201.09746.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скрипты в данной работе используют Python версию библиотеки [OpenCV](https://pypi.org/project/opencv-python/). Для запуска сред ATARI понадобится библиотека [The Arcade Learning Environment](https://github.com/Farama-Foundation/Arcade-Learning-Environment), а также библиотека [Shimmy](https://pypi.org/project/Shimmy/). Может потребоваться установка `ffmpeg` кодека. Для Unix-based операционной системы с пакетным менеджером APT может потребоваться следующая последовательность команд:\n",
    "```\n",
    "sudo apt-get install -y xvfb x11-utils ffmpeg python-opengl\n",
    "pip install pyglet pyvirtualdisplay opencv-python tqdm numpy moviepy gymnasium[atari]\n",
    "pip install gymnasium[accept-rom-license] #run 'pip install autorom; AutoROM --accept-license' if this fails\n",
    "```\n",
    "Для MacOS систем рекомендуется установить репозиторий [TFM](https://github.com/serrodcal-MII/TFM/tree/main) и выполнить инструкции из [README.md](https://github.com/serrodcal-MII/TFM/blob/main/README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from IPython.display import HTML\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.optim as opt\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "from atari_wrappers import nature_dqn_env\n",
    "from runners import EnvRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XVFB будет запущен в случае исполнения на сервере\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "nenvs = 64\n",
    "\n",
    "env = nature_dqn_env(\"SpaceInvadersNoFrameskip-v4\", nenvs=nenvs, summaries = \"Tensorboard\")\n",
    "                                                    # nenvs -- количество параллельно запущенных сред\n",
    "                                                    # данный параметр можно варьировать для баланса\n",
    "                                                    # производительность итерации/надёжность Монте-Карло оценок\n",
    "                                                    # помните: при уменьшении nenvs, возможно, придётся\n",
    "                                                    # увеличить количество итераций оптимизации\n",
    "n_actions = env.action_space.spaces[0].n\n",
    "obs, _ = env.reset()\n",
    "assert obs.shape == (nenvs, 4, 84, 84)\n",
    "assert obs.dtype == np.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующим шагом будет реализация модели, которая выводит логиты для категориального распределения на действия и оценку на значения $V$-функции ценности. Рекомендуется использовать архитектуру модели, представленной в публикации в журнале [Nature](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf) со следующей модификацией: вместо одного выходного слоя нужно сделать два слоя, принимающих в качестве входа выход предшествующего скрытого слоя. **Обратите внимание**, данная модель отличается от модели, предложенной в домашней работе по DQN. Рекомендуется использовать ортогональную инициализацию с параметром $\\sqrt{2}$ для ядер свёрток и инициализировать смещения нулями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_size_out(size, kernel_size, stride):\n",
    "    \"\"\"\n",
    "    общий принцип использования данной функции:\n",
    "    cur_layer_img_w = conv2d_size_out(cur_layer_img_w, kernel_size, stride)\n",
    "    cur_layer_img_h = conv2d_size_out(cur_layer_img_h, kernel_size, stride)\n",
    "    для вычисления размерности входа полносвязного слоя\n",
    "    \"\"\"\n",
    "    return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "\n",
    "\n",
    "class Flatten(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x.reshape(x.size(0), -1)\n",
    "\n",
    "\n",
    "class NatureModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, bsize, nchannels, height, width, n_actions, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=nchannels, out_channels=32, kernel_size=8, stride=4)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        cur_h, cur_w = conv2d_size_out(height, 8, 4), conv2d_size_out(width, 8, 4)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "        cur_h, cur_w = conv2d_size_out(cur_h, 4, 2), conv2d_size_out(cur_w, 4, 2)\n",
    "        self.conv3 = torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "        self.relu3 = torch.nn.ReLU()\n",
    "        cur_h, cur_w = conv2d_size_out(cur_h, 3, 1), conv2d_size_out(cur_w, 3, 1)\n",
    "        self.flatten = Flatten()\n",
    "        self.fc1 = torch.nn.Linear(in_features=64 * cur_h * cur_w, out_features=512)\n",
    "        self.relu4 = torch.nn.ReLU()\n",
    "        \n",
    "        self.fc_logits = torch.nn.Linear(in_features=512, out_features=n_actions)\n",
    "        self.fc_values = torch.nn.Linear(in_features=512, out_features=1)\n",
    "        \n",
    "        self._reset_parameters()\n",
    "    \n",
    "    def _reset_parameters(self):\n",
    "        torch.nn.init.orthogonal_(self.conv1.weight.data, gain=np.sqrt(2.))\n",
    "        self.conv1.bias.data.fill_(.0)\n",
    "        \n",
    "        torch.nn.init.orthogonal_(self.conv2.weight.data, gain=np.sqrt(2.))\n",
    "        self.conv2.bias.data.fill_(.0)\n",
    "        \n",
    "        torch.nn.init.orthogonal_(self.conv3.weight.data, gain=np.sqrt(2.))\n",
    "        self.conv3.bias.data.fill_(.0)\n",
    "        \n",
    "        torch.nn.init.orthogonal_(self.fc1.weight.data, gain=np.sqrt(2.))\n",
    "        self.fc1.bias.data.fill_(.0)\n",
    "        \n",
    "        torch.nn.init.orthogonal_(self.fc_logits.weight.data, gain=np.sqrt(2.))\n",
    "        self.fc_logits.bias.data.fill_(.0)\n",
    "        \n",
    "        torch.nn.init.orthogonal_(self.fc_values.weight.data, gain=np.sqrt(2.))\n",
    "        self.fc_values.bias.data.fill_(.0)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = torch.tensor(np.array(state), device=self.device, dtype=torch.float)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu4(x)\n",
    "        \n",
    "        logits = self.fc_logits(x)\n",
    "        value = self.fc_values(x)\n",
    "        \n",
    "        x.cpu()\n",
    "        del x\n",
    "        \n",
    "        return logits, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вам также потребуется определить и использовать политику, которая будет использовать модель выше. В то время как модель вычисляет логиты для всех действий сразу и оценку функции ценности, политика будет сэмплировать действия, а также будет вычислять их логарифм правдоподобия. Метод `Policy.act` должен возвращать словарь всех массивов, требуемых для взаимодействия со средой и обучения модели. Обратите внимание, что действия должны быть формата `Numpy.ndarray`, в то время как другие тензоры должны быть в формате, определяемом библиотекой глубокого обучения (`Torch.tensor`, например)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def act(self, inputs):\n",
    "        #<Реализуйте политику посредством прямого прохода модели, сэмплирования действий и вычисления их логарифмов правдоподобия>\n",
    "        # Должен возвращать dict с ключами ['actions', 'logits', 'log_probs', 'values'].\n",
    "        logits, values = self.model(inputs)\n",
    "        values = values.reshape(-1)\n",
    "        dist = torch.distributions.Categorical(logits = logits)\n",
    "        actions = dist.sample()\n",
    "        log_probs = dist.log_prob(actions)\n",
    "        actions = actions.cpu().numpy()\n",
    "        entropy = dist.entropy()\n",
    "        return {'actions': actions, 'logits': logits, 'log_probs': log_probs, 'values': values, 'entropy': entropy} \n",
    "    def reset(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее требуется передать среду и политику в исполнитель `EnvRunner`, который собирает частичные траектории из среды. Класс `EnvRunner` уже реализован за Вас.\n",
    "\n",
    "Данный исполнитель взаимодействует со средой заданное количество шагов и возвращает словарь, содержащий ключи:\n",
    "\n",
    "* 'observations' \n",
    "* 'rewards' \n",
    "* 'resets'\n",
    "* 'actions'\n",
    "* и другие ключи, определённые в `Policy`\n",
    "\n",
    "по каждому из этих ключей содержится Python `list` соответствующих результатов взаимодействий со средой указанной длины $T$ &mdash; размера частичной траектории."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того чтобы обучить часть модели, которая предсказывает ценности состояний, требуется вычислить целевые значения ценностей. В инстанцию класса `EnvRunner` можно подать при создании по аргументу `transforms` список вызываемых объектов (\"функций\"), которые последовательно будут применяться к частичным траекториям после сбора самих траекторий. Следовательно, требуется реализовать и использовать вызываемый (с определённым методом `__call__`) класс `ComputeValueTargets`. Формула для вычисления целевых значений ценности простая:\n",
    "\n",
    "$$\n",
    "\\hat v(s_t) = \\sum_{t'=0}^{T - 1}\\gamma^{t'}r_{t+t'} + \\gamma^T \\hat{v}(s_{t+T}),\n",
    "$$\n",
    "\n",
    "Однако, не забудьте в реализации использовать `trajectory['resets']` флаги для проверки того, следует ли добавить целевые значения ценности на следующем шаге при вычислении целевых значений ценности на текущем шаге. У вас также имеется доступ к `trajectory['state']['latest_observation']` для получения последнего наблюдения в частичной траектории &mdash; $s_{t+T+1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputeValueTargets:\n",
    "    def __init__(self, gamma=0.99, device=torch.device('cuda:0')):\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "\n",
    "    def __call__(self, trajectory):\n",
    "        # Данный метод должен модифицировать траекторию trajectory на месте через добавление\n",
    "        # итеративно заполненного списка по ключу 'value_targets'.\n",
    "        rewards = torch.tensor(np.stack(trajectory['rewards']), device = self.device, dtype=torch.float32)\n",
    "        value_targets = torch.tensor(np.stack(trajectory['rewards']), device = self.device, dtype=torch.float32)\n",
    "        resets = torch.tensor(np.stack(trajectory['resets']), device = self.device, dtype=torch.float32)\n",
    "\n",
    "        values = torch.stack(trajectory['values'])\n",
    "        \n",
    "        value_targets[-1] = values[-1] * (1 - resets[-1]) +  rewards[-1] * resets[-1]\n",
    "        for t in range(trajectory['state']['env_steps'] - 2, -1, -1):\n",
    "            value_targets[t] = value_targets[t] + (value_targets[t + 1] * self.gamma) * (1 - resets[t])\n",
    "        trajectory['value_targets'] = value_targets\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После вычисления целевых значений ценности требуется преобразовать списки результатов взаимодействия со средой в тензоры с первой компонентой размерности `batch_size`, равной призведению `T * nenvs`, то есть требуется свести в одну компоненту первые две компоненты размерности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergeTimeBatch:\n",
    "    \"\"\"\n",
    "    Сращивает первые две оси, обычно отвечающие за время и за инстанцию среды, соответственно.\n",
    "    \"\"\"\n",
    "    def __call__(self, trajectory):\n",
    "        # Модификация траектории на месте. \n",
    "        device = trajectory['value_targets'].device\n",
    "        trajectory['rewards'] = torch.tensor(np.stack(trajectory['rewards']), device = device, dtype=torch.float32).reshape(-1)\n",
    "        trajectory['resets'] = torch.tensor(np.stack(trajectory['resets']), device = device, dtype=torch.float32).reshape(-1)\n",
    "        trajectory['values'] = torch.stack(trajectory['values']).to(device).reshape(-1)\n",
    "        trajectory['entropy'] = torch.stack(trajectory['entropy']).to(device).reshape(-1)\n",
    "        trajectory['value_targets'] = trajectory['value_targets'].reshape(-1)\n",
    "        trajectory['log_probs'] = torch.stack(trajectory['log_probs']).to(device).reshape(-1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 617 # на своё усмотрение можно выбрать другой seed\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "nsteps=5\n",
    "model = NatureModel(*obs.shape, n_actions, device)\n",
    "model.to(device)\n",
    "policy = Policy(model)\n",
    "torch.save(model.state_dict(), 'agent_actor_model_init.pt')\n",
    "#model.load_state_dict(torch.load('agent_actor_model_init.pt'))\n",
    "runner = EnvRunner(env, policy, nsteps=5, # nsteps -- длина частичной траектории\n",
    "                                          # уменьшение nsteps может привести к вынужденному увеличению\n",
    "                                          # количества итераций оптимизации\n",
    "                   transforms=[ComputeValueTargets(device=device),\n",
    "                               MergeTimeBatch()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Настало время реализовать сам алгоритм Advantage-Actor Critic (A2C). Его псевдокод можно посмотреть в [конспектах лекций (Раздел 5.2.5)](https://arxiv.org/pdf/2201.09746.pdf), в публикции [Mnih et al. 2016](https://arxiv.org/abs/1602.01783) и в [лекции](https://www.youtube.com/watch?v=Tol_jw5hWnI&list=PLkFD6_40KJIxJMR-j5A1mkxK26gh_qg37&index=20) Сергея Левина."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C:\n",
    "    def __init__(self,\n",
    "               policy,\n",
    "               optimizer,\n",
    "               value_loss_coef=.25,\n",
    "               entropy_coef=1e-2,\n",
    "               max_grad_norm=.5):\n",
    "        self.policy = policy\n",
    "        self.optimizer = optimizer\n",
    "        self.value_loss_coef = value_loss_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "    def policy_loss(self, trajectory):\n",
    "        # Тут нужно вычислить Advantages. \n",
    "        #<TODO: реализовать>\n",
    "        advantages = trajectory['value_targets'] -  trajectory['values']\n",
    "        advantages = advantages.detach()\n",
    "        return -(trajectory['log_probs'] * advantages).mean()\n",
    "    def value_loss(self, trajectory):\n",
    "        #<TODO: реализовать>\n",
    "        return torch.nn.MSELoss()(trajectory['values'], trajectory['value_targets'].detach())\n",
    "\n",
    "    def loss(self, trajectory):\n",
    "        entropy = trajectory['entropy'].mean()\n",
    "        \n",
    "        return self.value_loss_coef * self.value_loss(trajectory) + self.policy_loss(trajectory) - (self.entropy_coef * entropy)\n",
    "\n",
    "    def step(self, trajectory):\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.loss(trajectory)\n",
    "        loss.backward()\n",
    "        norm = torch.nn.utils.clip_grad_norm_(model.parameters(), self.max_grad_norm)\n",
    "        self.optimizer.step()\n",
    "        return loss, norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь можно непосредственно обучить Вашу модель. С разумно подобранными гиперпараметрами обучение на одной GTX1080 на протяжении 10 миллионов шагов суммарно со всех батчированных сред (что переводится примерно в 5 часов работы) должно быть возможно достигнуть *среднюю исходную награду за 100 последних эпизодов* (значение переменной в `Tensorboard` по ключу `reward_mean_100`, усреднение берётся по 100 последним эпизодам в каждой среде в батче) **не меньше 600**. Это и будет считаться успешным результатом работы алгоритма `A2C`.\n",
    "\n",
    "**Внимание!** При *коректной* имплементации алгоритма *обоснованное* преодоление порога **400** по `reward_mean_100` *транслируется в* **8 баллов за задание**, *обоснованное* преодоление порога **600** по `reward_mean_100` на *корректно* написанном алгоритме обучения `A2C` *транслируется в* **10 баллов за задание**.\n",
    "\n",
    "Вам так же, по возможности, рекомендуется отобразить данную величину относительно `runner.step_var` &mdash; количества взаимодействий со всеми средами. Также очень рекомендуется предоставить графики следующих показателей (полезно для отладки кода):\n",
    "* [Коэффициент детерминации](https://en.wikipedia.org/wiki/Coefficient_of_determination) между целевыми значениями ценности и их предсказаниями\n",
    "* Энтропия политики $\\pi$\n",
    "* Функция потерь ценности (Value loss)\n",
    "* Функция потерь политики (Policy loss)\n",
    "* Целевые значения ценности (Value targets)\n",
    "* Предсказания значений ценности (Value predictions)\n",
    "* Норма градиента\n",
    "* Advantages\n",
    "* Общая функция потерь (A2C loss)\n",
    "\n",
    "В качестве оптимизатора рекомендуется взять метод [RMSProp](https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html) с [линейным убыванием шага](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LambdaLR.html), начиная с 7e-4 до 0, константой сглаживания (alpha в PyTorch и decay в TensorFlow), равной 0.99 и epsilon, равным 1e-5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T01:43:12.609689Z",
     "start_time": "2024-04-16T01:43:12.605243Z"
    }
   },
   "source": [
    "Запуск Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-83c60b6f44d5fbbc\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-83c60b6f44d5fbbc\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ddd673b71fc470ab1d0d13a4df12ada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torcheval.metrics\n",
    "writer = SummaryWriter('./logs/run-1')\n",
    "lr = 7e-4\n",
    "alpha=0.99\n",
    "eps=1e-05\n",
    "total_steps = int(80e6)\n",
    "eval_freq = 100\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr = lr, alpha=alpha, eps=eps)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda t: (total_steps - t) / total_steps)\n",
    "a2c = A2C(policy, optimizer, value_loss_coef=.25, entropy_coef=1e-2, max_grad_norm=.5)\n",
    "r2metric =  torcheval.metrics.R2Score()\n",
    "batch_size = nenvs*nsteps\n",
    "for step in tqdm(range(total_steps//batch_size+ 1)):\n",
    "    trajectory = runner.get_next()\n",
    "    loss, norm = a2c.step(trajectory)\n",
    "    scheduler.step()\n",
    "    if(step%eval_freq == 0):\n",
    "        writer.add_scalar('Eval/Entropy', trajectory['entropy'].mean(), step)\n",
    "        writer.add_scalar('Eval/value_loss', a2c.value_loss(trajectory), step)\n",
    "        writer.add_scalar('Eval/policy_loss', a2c.policy_loss(trajectory), step)\n",
    "        writer.add_scalar('Eval/loss', a2c.loss(trajectory), step)\n",
    "        r2metric.update(trajectory['values'].cpu(), trajectory['value_targets'].detach().cpu())\n",
    "        writer.add_scalar('Eval/R2Metric', r2metric.compute(), step)\n",
    "        writer.add_scalar('Eval/gradnorm', norm, step)\n",
    "        torch.save(model.state_dict(), 'agent_actor_model.pt')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+01d3e3a)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "test_env = nature_dqn_env(\"SpaceInvadersNoFrameskip-v4\", nenvs=None, clip_reward=False, summaries=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_lives = 3\n",
    "\n",
    "def evaluate(env, agent, n_games=1, greedy=False, t_max=10000):\n",
    "    \"\"\"\n",
    "    Играем n_games игр до конца.\n",
    "    В случае жадной политики, выбираем действия как argmax(qvalues).\n",
    "    Возвращаем среднюю награду.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for _ in range(n_games):\n",
    "        s, _ = env.reset()\n",
    "        reward = 0\n",
    "        for _ in range(t_max):\n",
    "            output = agent.act([s])\n",
    "            action = output['logits'].argmax(dim=-1).item() if greedy else output['actions'][0]\n",
    "            s, r, terminated, truncated, _ = env.step(action)\n",
    "            reward += r\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        rewards.append(reward)\n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sasha/anaconda3/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:364: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/sasha/RL4/videos/False/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/sasha/RL4/videos/False/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/sasha/RL4/videos/False/rl-video-episode-0.mp4\n",
      "Moviepy - Building video /home/sasha/RL4/videos/False/rl-video-episode-1.mp4.\n",
      "Moviepy - Writing video /home/sasha/RL4/videos/False/rl-video-episode-1.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/sasha/RL4/videos/False/rl-video-episode-1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/sasha/RL4/videos/False/rl-video-episode-2.mp4.\n",
      "Moviepy - Writing video /home/sasha/RL4/videos/False/rl-video-episode-2.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/sasha/RL4/videos/False/rl-video-episode-2.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# запись эпизодов\n",
    "with RecordVideo(\n",
    "    env=test_env,\n",
    "    video_folder=\"./videos/False\",\n",
    "    episode_trigger=lambda episode_number: True\n",
    ") as env_monitor:\n",
    "    sessions = [evaluate(env_monitor, policy, n_games=n_lives, greedy=False) for _ in range(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"videos/False/rl-video-episode-1.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_paths = sorted([s for s in Path('videos', 'False').iterdir() if s.suffix == '.mp4'])\n",
    "video_path = video_paths[1]\n",
    "data_url = str(video_path)\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(data_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/sasha/RL4/videos/True/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/sasha/RL4/videos/True/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/sasha/RL4/videos/True/rl-video-episode-0.mp4\n",
      "Moviepy - Building video /home/sasha/RL4/videos/True/rl-video-episode-1.mp4.\n",
      "Moviepy - Writing video /home/sasha/RL4/videos/True/rl-video-episode-1.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/sasha/RL4/videos/True/rl-video-episode-1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/sasha/RL4/videos/True/rl-video-episode-2.mp4.\n",
      "Moviepy - Writing video /home/sasha/RL4/videos/True/rl-video-episode-2.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/sasha/RL4/videos/True/rl-video-episode-2.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# запись эпизодов\n",
    "with RecordVideo(\n",
    "    env=test_env,\n",
    "    video_folder=\"./videos/True\",\n",
    "    episode_trigger=lambda episode_number: True\n",
    ") as env_monitor:\n",
    "    sessions = [evaluate(env_monitor, policy, n_games=n_lives, greedy=True) for _ in range(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"videos/True/rl-video-episode-1.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_paths = sorted([s for s in Path('videos', 'True').iterdir() if s.suffix == '.mp4'])\n",
    "video_path = video_paths[1]\n",
    "data_url = str(video_path)\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(data_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Информация об обучении"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прикрепление скриншотов графиков обучения модели в `Tensorboard` ниже является обязательным. Для доступа к `Tensorboard` запустите из командной строки в одной директории с данным ноутбуком следующую команду:\n",
    "```\n",
    "tensorboard --logdir logs --port 6006\n",
    "```\n",
    "В результате вывод в командную строку укажет, по какому адресу можно подсоединиться к инстанции `Tensorboard`, например, по адресу `http://localhost:6006/`. Оттуда можно и сделать скриншоты, демонстрирующие результаты обучения модели. Сами скриншоты с именем файла `image_name_x.png` для удобства лучше сохранить в директорию `./img`, откуда можно легко их прикреплять в `Markdown-клетках` ниже по команде со следующей конструкцией:\n",
    "```\n",
    "<img src=./img/image_name_x.png width=640>\n",
    "```\n",
    "Тут также требуется подписать изображения и дать небольшой комментарий по каждому скриншоту, что на нём описано.\n",
    "\n",
    "**Внимание!** В случае перезапуска процедуры обучения модели рекомендуется удалить директорию `./logs` вместе с её содержимым перед непосредственным перезапуском, чтобы не испортить отображающиеся графики в `Tensorboard`.\n",
    "\n",
    "**Совет.** При работе в Google Colab можно просто скачать директорию `./logs` и уже локально запустить `Tensorboard` для снятия скриншотов. Также можно обученного агента сохранить, скачать и локально на cpu запустить для записи роликов (для этого понадобится самостоятельно прописать код сохранения и загрузки модели в ноутбук из [файла](https://pytorch.org/tutorials/beginner/saving_loading_models.html)).\n",
    "\n",
    "**Внимание!** Посылку для сдачи задания требуется оформить в виде `.zip` архива, в котором будут *данный ноутбук*, использованные для его работы *скрипты*, *директории* `./videos`, `./logs` и `./img` с содержимым. Только так и не иначе!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Вставьте в данную ячейку свой ответ, подкреплённый скриншотами__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=./img/episode_legth_max_reward.png width=640>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Длина эпизода коррелирует с оценкой reward_mean_100. Как видим максимальная награда грастет и на порядок превышает среднюю оценку, что говорит о широком разнообразии просматриваемых решений, при данной политики, при это модел ьпродолжает обучаться"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=./img/min_reward_reward_mean_100.png width=640>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Минимальная награда не меняется особо и находится на около 0 уровне, что говорит о том, что политика не вырожддается и мы рассматриваем даже самые непродуктивные случаи, в угоду возможностям исследования. Средняя награда говорит о том что найден примерный балн между исследванием и наградами. Модель имеет достаточно широкую свободу для исследований, при этом не забывая уделять время продуктивным эпизодам, чт ов среднем дает нелохой темп обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=./img/Entropy_R2Metric.png width=640>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Энтропия выступет в качестве меры случайности policy-net, в целом постепенно находя оптимальную политику ее случайность уменьшается, но пока она далека от вырожденного детерминированного случая. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R2-Metric изначально у меня получалась около 0, что говорило об ошибке в коде при подсчете target_value. В идеале target_value и value должны оставаться скоррелированны, т.к. наиболее вероятный исход, что предыдуще занчение value совпадает с текущим (т.к. награда у нас всего лишь 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=./img/gradnorm_loss.png width=640>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradnorm в целом находится меньше 0.5, но под конец градиент пришлось клиповать. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=./img/policy_loss_value_loss.png width=640>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для нахождения баланса между value_loss и policy_loss нужно чтобы каждый из loss'ов коррелировал с loss, т.е. чтобы они давалаи примено одинаковый вклад в него. Таким образом был подобран весовой коэффициент"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
